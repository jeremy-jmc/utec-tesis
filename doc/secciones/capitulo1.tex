
\chapter{THEORETICAL FRAMEWORK}


\section{Spatio-Temporal Data}

The spatio-temporal data is a type of data that contains information about the spatial and temporal dimensions of an event or phenomenon. 
This dual indexing enables the representation of complex relationships between spatial and temporal elements, allowing for a more comprehensive understanding of the data.

The spatio-temporal data can be formalized as a tuple $ST = \{ (s_i, t_j, X_{ij} | s_i \in S , t_j \in T , X_{ij} \in \mathbb{R}^d )\}$, where $S$ is the spatial domain, $T$ is the temporal domain, and $X_{ij}$ signifies the observed attributes at location $s_i$ and time $t_j$.
 




%\section{Textual Graphs}

%\subsection{Knowledge Graphs}

%\subsection{Spatial Graphs}

%\subsection{Temporal Graphs}

%\subsection{Spatial-Temporal Graphs}





\section{Language Models (LLMs)}
% BERT

Large Language Models (LLMs) are built upon the transformer architecture, originally introduced by \cite{vaswani2023attentionneed}. This architecture revolutionized natural language processing (NLP) by replacing recurrent neural networks (RNNs) with self-attention mechanisms, enabling models to process entire sequences in parallel rather than sequentially.

The core components of the transformer architecture include:
\begin{itemize}
    \item \textbf{Self-attention mechanisms}: Allow the model to assess the importance of different words within a given context.
    \item \textbf{Multi-head attention}: Enables the model to attend to information from multiple representation subspaces simultaneously.
    \item \textbf{Feed-forward neural networks}: Apply non-linear transformations to the attention outputs.
    \item \textbf{Layer normalization}: Helps stabilize and accelerate the training process.
    \item \textbf{Positional encoding}: Introduces information about the order of tokens in a sequence.
\end{itemize}

Notable examples of LLMs include OpenAI's GPT-3, Google's BERT and T5, and Meta's RoBERTa. These models have set new benchmarks across a variety of NLP tasks such as text classification, machine translation, and summarization. While all are built on the transformer architecture, they differ in their design choices and training objectives. For instance, BERT uses a masked language modeling objective, where random tokens in a sentence are masked and the model learns to predict them using surrounding context. In contrast, GPT-3 follows an autoregressive training strategy, generating one token at a time based on previously generated tokens.

The theoretical foundation of LLMs lies in probabilistic language modeling. Given a sequence of tokens \( x = (x_1, x_2, \dots, x_n) \), a language model aims to estimate either the joint probability \( P(x) \) or the conditional probability of the next token \( P(x_n \mid x_1, \dots, x_{n-1}) \). The model is trained to assign higher probabilities to sequences that are more likely to appear in natural language, based on patterns learned from large-scale datasets. This is made possible by the transformer's use of self-attention and feed-forward layers, which together capture complex dependencies across tokens. In particular, positional encodings and multi-head attention enable LLMs to model long-range relationships—something that earlier architectures like RNNs and LSTMs struggled to achieve effectively.


\section{Prompting}

Basically prompting is a technique used to guide the behavior of LLMs by providing them with specific instructions or context. 
This can be done through various methods, such as zero-shot prompting, few-shot prompting, and chain-of-thought prompting. 
Each method has its own advantages and disadvantages, depending on the task at hand and the desired outcome.


\subsection{Prompt Engineering}

% https://www.promptingguide.ai/ cite from here 
Prompt engineering is an emerging field focused on crafting and refining prompts to make the most effective use of language models (LMs) 
across diverse applications and research areas. It plays a crucial role in enhancing our understanding of the strengths and limitations of large language models (LLMs). 
Beyond simply writing prompts, prompt engineering involves a broad set of techniques essential for building with, interacting with, and expanding the capabilities of LLMs. 
It also contributes to improving model safety and enables the integration of specialized knowledge and functionalities into LLM-based systems


\begin{itemize}
    \item \textbf{Zero-shot prompting}: Involves providing the model with a task description or question without any examples. The model is expected to generate a response based solely on its pre-existing knowledge and understanding of the task.
    \item \textbf{Few-shot prompting}: As mentioned by \cite{fewshot2020}, this technique involves providing the model with a few examples of the desired output format or task. This helps the model understand the context and generate more accurate responses.
    \item \textbf{Chain-of-thought prompting}: Firstly introduced by \cite{chainofthought2023}, encourages the model to generate intermediate reasoning steps before arriving at a final answer. This approach has been shown to improve performance on complex tasks, such as arithmetic reasoning and logical inference.
\end{itemize}










\section{Retrieval Augmented Generation (RAG)}

Retrieval-Augmented Generation (RAG), first introduced by \cite{RAG2021}, is a framework that combines the strengths of retrieval-based and generative approaches for question answering. Its core idea is to enhance the generative capabilities of language models by incorporating relevant information retrieved from external knowledge sources, rather than relying solely on parametric knowledge (i.e., information encoded in the model's weights).

The RAG architecture consists of two primary components: 
\begin{itemize}
     
    \item \textbf{Retriever}: This component retrieves relevant documents or passages from a large corpus based on the input query. It typically employs models such as BM25 or dense retrieval methods to identify the most relevant information. 
    \item \textbf{Generator}: Once the retriever has collected the relevant documents, the generator—usually a transformer-based language model—takes them as input to produce a coherent and contextually appropriate response. The generator can be fine-tuned for specific tasks to further improve its performance. 
\end{itemize}

In recent years, RAG has gained considerable attention in the NLP community for its ability to generate high-quality responses. This has spurred the development of new techniques to improve both retrieval precision and generation quality. For instance, \cite{modularRAG2024} introduces a modular RAG framework that enables the integration of diverse components across different stages of the pipeline. These include techniques such as query expansion, reformulation, and transformation in the pre-retrieval phase, as well as the use of large language models as judges in the post-retrieval phase to evaluate if the response is enoughly complete.
This modular design encourages flexibility and supports experimentation with novel combinations and architectural variations.







% \section{Metrics}

% \subsection{Accuracy}

% \subsection{Precision}

% \subsection{Recall}

% \subsection{F1}

% \subsection{BERTScore}

% \subsection{Hit Rate}

% \subsection{Mean Reciprocal Rank (MRR)}



