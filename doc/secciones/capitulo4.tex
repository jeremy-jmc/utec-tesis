\lstdefinestyle{pythoncode}{
  language=Python,
  basicstyle=\scriptsize\ttfamily,
  backgroundcolor=\color{gray!5},
  keywordstyle=\color{blue},
  commentstyle=\color{green!50!black},
  stringstyle=\color{orange},
  numberstyle=\tiny\color{gray},
  numbers=left,
  stepnumber=1,
  numbersep=8pt,
  frame=single,
  breaklines=true,
  captionpos=b,
  showstringspaces=false,
  tabsize=4
}
% Ajuste opcional del formato de captions
\captionsetup[lstlisting]{font=small,labelfont=bf}



\chapter{ EXPERIMENTS AND RESULTS}

This chapter presents the outcomes derived from applying the methodology detailed in the preceding chapter. We begin by describing the experimental setup and specifying the hyperparameters employed. Next, we report the results from the evaluated models, covering both quantitative and qualitative analyses. Lastly, we discuss the significance of these findings and include case studies to demonstrate the practical relevance of our results.

\section{Experimental setup}

\section{Hyperparameters}


The selection of key hyperparameters for training was guided by empirical observation and best practices in fine-tuning large language models:
\begin{itemize}
    
\item \textbf{Warmup Ratio:} We initially set the warmup ratio to 0.03; however, this led to instability during the early stages of training, with noticeable spikes in the loss. Increasing the warmup ratio to 0.1 significantly improved training stability, consistent with findings in transformer-based models such as RoBERTa, where extended warmup periods are known to facilitate smoother convergence \citep{liu2019robertarobustlyoptimizedbert}.

\item \textbf{Weight Decay:} To mitigate overfitting and enhance generalization, especially given the relatively small size of our dataset, we applied a weight decay of 0.01. This choice aligns with established practices in training deep neural networks on limited data, where appropriate regularization is crucial for model robustness \citep{brainacgan}.

\item \textbf{Learning Rate:} The learning rate was initially set to 2e-3, but this configuration resulted in poor convergence during training. Considering that we employed LoRA for fine-tuning the LLaMA model, we reduced the learning rate to 7e-5. This lower rate aligns with commonly adopted values for adapting pre-trained large language models using parameter-efficient techniques, and it yielded significantly improved convergence behavior \citep{zhou2024automixqselfadjustingquantizationhigh}.

\end{itemize}}

\section{Quantitative Results}

% TODO: TABLE 1 comparing Phi3-mini4k-instruct, Llama3-8B-Instruct, baseline and trained models along with Closed-Source models like GPT-4o-models, pass@k with k = 1 (with greedy decoding), 4, 16 (multinomial sampling)

\begin{table}[h!]
\centering
\caption{Evaluation Metrics finetuned LLaMA3: Mean, Std, and Median (Test Set)}
% \resizebox{\textwidth}{!}
{%
\begin{tabular}{l|ccc}
\toprule
\textbf{Metric} & \textbf{Mean} & \textbf{Std} & \textbf{Median} \\
\midrule
perc\_error@ & 0.233 & 0.299 & 0.125 \\
pass@k    & 0.742 & 0.437 & 1.0   \\
pass\_over\_k  & 0.460 & 0.410 & 0.406 \\
code\_bleu@k  & 0.355 & 0.044 & 0.353 \\
\bottomrule
\end{tabular}
}
\label{tab:metrics_generated}
\end{table}

\begin{table}[h!]
\centering
\caption{Evaluation Metrics for o4-mini: Mean, Std, and Median (Test Set)}
{%
\begin{tabular}{l|ccc}
\toprule
\textbf{Metric} & \textbf{Mean} & \textbf{Std} & \textbf{Median} \\
\midrule
perc\_error@k     & 0.003 & 0.014 & 0.0   \\
pass\_at@k        & 0.303 & 0.459 & 0.0   \\
pass\_over\_k      & 0.063 & 0.175 & 0.0   \\
code\_bleu@k      & 0.370 & 0.054 & 0.375 \\
\bottomrule
\end{tabular}
}
\label{tab:metrics_new_data}
\end{table}


\section{Discussion}

Based on the observed metrics, several conclusions emerge regarding the performance of the fine-tuned LLaMA3‑8B‑Instruct model versus the base o4‑mini. First, LLaMA3‑8B‑Instruct exhibits a higher perc\_error@k than o4‑mini; yet it significantly outperforms o4‑mini in both pass@k and pass\_over\_k. This indicates that, despite a slightly lower rate of successful compilation, the fine‑tuned model produces more precise and semantically relevant solutions. In contrast, o4‑mini’s better perc\_error@k can be misleading: it often compiles code that runs but fails to solve the problem correctly.

This discrepancy is likely rooted in architectural and training differences. o4‑mini is believed to use a Mixture‑of‑Experts (MoE) setup with approximately 40B total parameters—only about 8B of which are active per inference—whereas LLaMA3‑8B‑Instruct is a dense model with all 8B parameters actively contributing . While MoE architectures provide access to a broader parameter pool, this capacity advantage does not automatically translate to superior domain-specific performance. In contrast, the fully fine‑tuned LLaMA model has internalized domain-specific patterns—such as those needed to manipulate crime data (locations, dates, crime types, etc.)—enabling it to generate more accurate and relevant code.
\section{Qualitative Analysis}

% \section{Discussion}

\section{Case studies}

\subsection{Case I: Counterfactual Evaluation of Targeted Interventions in Crime Hotspots of Guangdong Province}

Zhongshan is a prefecture-level city in Guangdong province with a population of approximately 4.4 million as of 2020 that has pursued aggressive data-driven policing strategies. According to official reports, Zhongshan experienced notable crime declines in recent years. Mid-2020 figures indicated a 37\% drop in serious violent crimes and a 46\% drop in theft and robbery cases compared to the previous year \cite{Zhongshan2020}. This context of active crime prevention underscores the importance of analyzing Zhongshan's crime data to understand underlying patterns and inform future policies.

Spatio-temporal crime analysis serves as a key tool for modern law enforcement. By mapping incident locations and times, analysts can identify hotspots and temporal trends that guide resource allocation. Early studies such as the STAC program demonstrated that outlining incident boundaries and measuring crime counts in specific areas can reveal concentration patterns for tactical decisions. More recently, forecasting models have been shown to enhance public safety by predicting future crime distributions and trends \cite{Li2024UrbanGPT}. Together, these approaches illustrate how comprehensive analysis of crime datasets can support both forensic investigations and strategic planning.

Counterfactual modeling extends this analysis by posing hypothetical scenarios that explore alternative outcomes. Counterfactual analysis allows decision-makers to compare actual outcomes with those that would have occurred under alternate conditions \cite{CONSTANTINOU201641}. In forensic or public safety settings, this means estimating the effect of interventions that did not actually happen. Analysts might examine how overall crime rates would change if a particular category of incidents had been prevented. By modeling such scenarios, such as removing all night-time thefts from the record, researchers can infer the causal impact of those incidents. These hypothetical comparisons provide insight into the potential efficacy of different policies or data reporting practices without requiring real-world experimentation \cite{CONSTANTINOU201641}.

In this case study, we propose leveraging a fine-tuned large language model (LLM) as an interactive reasoning tool over Zhongshan's spatio-temporal crime data. Recent work has shown that integrating LLMs with geographic information systems enables natural-language querying of spatial data \cite{William2025}. An LLM can be connected to a GIS so that users can ask plain-language questions about maps and patterns, with the model translating these queries into actionable insights \cite{William2025}. This approach bridges textual and spatial information, expediting analysis and discovery in spatial datasets \cite{William2025}. A fine-tuned LLM could therefore generate and reason about complex questions regarding the Zhongshan crime dataset, acting as a digital assistant that translates data-driven scenarios into understandable explanations.
\noindent In the following sections, we present three counterfactual questions for the LLM to address:
\begin{enumerate}
  \item What percentage reduction in overall crime could be achieved if all night-time thefts in Zhongshan during 2019 were prevented?
  \item How would the average crime in neighboring counties (within a 70~km radius of [22.516998, 113.392723]) change if crime at that central hotspot were reduced by 20\%?
  \item How would the top 10 safest counties in Guangdong change if crime reporting increased by 15\% in Zhongshan during 2019?
\end{enumerate}

\subsubsection{Case I.1:}
\begin{itemize}
    \item \textbf{Question:} "What percentage reduction in overall crime could be achieved if all night-time thefts in Guangdong Province during 2019 were prevented?"
    \item \textbf{Model Code Generation:} One of the six code generations that were made by the model is shown below. 

    \lstinputlisting[
    style=pythoncode,    
    label={lst:q1}
    ]{codigos/q1.py}



    \item \textbf{Model Response:} 
    
    \begin{tcolorbox}[colback=white!95!black, colframe=black, title=Q1 Response, fontupper=\scriptsize]

    "The province of Guangdong in China reported 1376 total crimes in 2019. Out of those, 633 crimes occurred at night. If all night-time crimes were prevented, the overall crime rate would decrease by 46.00\%. This is because 46.00\% of the total crimes in 2019 were night-time crimes."
    \end{tcolorbox}

\subsubsection{Case I.2:}
\begin{itemize}
    \item \textbf{Question:} "How would the average crime in neighboring counties (within a 70 km radius of [22.516998, 113.392723]) change if crime at that central point were reduced by 20\%?"
    \item \textbf{Model Code Generation:} One of the six code generations that were made by the model is shown below. 


    \lstinputlisting[
    style=pythoncode,    
    label={lst:q2}
    ]{codigos/q2.py}

  \item \textbf{Model Response:} 
    

    \begin{tcolorbox}[colback=white!95!black, colframe=black, title=Q2 Response, fontupper=\scriptsize]
    
    The average crime rate in neighboring counties would decrease by 13.85\% if crime at the central point were reduced by 20\%. 

    This is because the simulated reduced crime rate in neighboring counties decreased from 4874.79 to 4199.85.
    
    \end{tcolorbox}

\end{itemize}
\subsubsection{Case I.3:}
\begin{itemize}
    \item \textbf{Question:} "Considering the top 10 of the safest all counties in Guangdong Province, How would this top 10 change if the crime reporting increased by 15\% in Zhongshan City in 2019?"
    \item \textbf{Model Code Generation:} One of the six code generations that were made by the model is shown below.

    \lstinputlisting[
    style=pythoncode,    
    label={lst:q3}
    ]{codigos/q3.py}
    
    
    \item \textbf{Model Response:} 
\begin{tcolorbox}[colback=white!95!black, colframe=black, title=Q3 Response, fontupper=\scriptsize]
Based on the provided code outputs, the current top 10 safest counties in Guangdong Province for 2019 are:

\begin{enumerate}
  \item Bao'an District with 298 crimes
  \item Dongguan City with 139 crimes
  \item Baiyun District with 113 crimes
  \item Zhongshan City with 80 crimes
  \item Longgang District with 78 crimes
  \item Panyu District with 62 crimes
  \item Shunde District with 61 crimes
  \item Chancheng District with 56 crimes
  \item Nanshan District with 55 crimes
  \item Haizhu District with 49 crimes
\end{enumerate}

If the crime reporting in Zhongshan City increased by 15\% in 2019, the new top 10 safest counties would be:

\begin{enumerate}
  \item Bao'an District with 298 crimes
  \item Dongguan City with 139 crimes
  \item Baiyun District with 113 crimes
  \item Zhongshan City with 92 crimes (after 15\% increase)
  \item Longgang District with 78 crimes
  \item Panyu District with 62 crimes
  \item Shunde District with 61 crimes
  \item Chancheng District with 56 crimes
  \item Nanshan District with 55 crimes
  \item Haizhu District with 49 crimes
\end{enumerate}

The change in ranking is due to Zhongshan City moving from 4th to 4th place after the increase in crimes.
\end{tcolorbox}

    
\end{itemize}

\subsection{Case II: Geospatial Queries}


% \chapter{MARCO METODOLÓGICO}

% Inicie aquí el texto, utilizando sangría de 1.25 cm. en el primer
% párrafo. Continúe el segundo párrafo.

% Continúe el segundo párrafo

% \section{Primer subtítulo}

% En la actualidad, las unidades de información hacen frente a muchos cambios
% debido a los avances tecnológicos, explosión informativa, nuevos recursos y
% soportes, por lo cual se implementan servicios innovadores que les permitan a
% sus usuarios tener acceso a muchas fuentes de información. Para asegurar el
% acceso y uso de los servicios los usuarios requieren poseer una serie de
% habilidades que les permitan identificar, recuperar, manejar, discernir,
% organizar, utilizar y comunicar la información de manera eficaz para la toma de
% decisiones.

% Entre las causas se pueden considerar la falta de tiempo asignado al taller,
% las limitaciones en cuanto a laboratorios, el poco personal, la falta de una
% correcta selección de los contenidos, así como, una calificación que asegure el
% cumplimiento de los objetivos.

\section{Resume}

