\lstdefinestyle{pythoncode}{
  language=Python,
  basicstyle=\tiny\ttfamily,
  backgroundcolor=\color{gray!5},
  keywordstyle=\color{blue},
  commentstyle=\color{green!50!black},
  stringstyle=\color{orange},
  numberstyle=\tiny\color{gray},
  numbers=left,
  stepnumber=1,
  numbersep=8pt,
  frame=single,
  breaklines=true,
  captionpos=b,
  showstringspaces=false,
  tabsize=4,
  baselinestretch=0.8,
  lineskip=-1pt
}
% Ajuste opcional del formato de captions
\captionsetup[lstlisting]{font=small,labelfont=bf}



\chapter{ EXPERIMENTS AND RESULTS}

This chapter presents the outcomes derived from applying the methodology detailed in the preceding chapter. We begin by specifying the hyperparameters employed during the training phase. Next, we report the results from the evaluated models, covering both quantitative and qualitative analyses. Lastly, we discuss the significance of these findings and include case studies to demonstrate the practical relevance of our results.

\section{Hyperparameters}


The selection of key hyperparameters for training was guided by empirical observation and best practices in fine-tuning large language models:

\begin{itemize}
  \item \textbf{Warmup Ratio:} We initially set the warmup ratio to 0.03; however, this led to instability during the early stages of training, with noticeable spikes in the loss. Increasing the warmup ratio to 0.1 significantly improved training stability, consistent with findings in transformer-based models such as RoBERTa, where extended warmup periods are known to facilitate smoother convergence \citep{liu2019robertarobustlyoptimizedbert}.
  \item \textbf{Weight Decay:} To mitigate overfitting and enhance generalization, especially given the relatively small size of our dataset, we applied a weight decay of 0.01. This choice aligns with established practices in training deep neural networks on limited data, where appropriate regularization is crucial for model robustness \citep{brainacgan}.
  \item \textbf{Learning Rate:} The learning rate was initially set to 2e-3, but this configuration resulted in poor convergence during training. Considering that we employed LoRA for fine-tuning the LLaMA model, we reduced the learning rate to 7e-5. This lower rate aligns with commonly adopted values for adapting pre-trained large language models using parameter-efficient techniques, and it yielded significantly improved convergence behavior \citep{zhou2024automixqselfadjustingquantizationhigh}.
\end{itemize}

The other hyperparameters (such as LoRa range and batch size) were chosen according to hardware limitations. Full detail in Table \ref{tab:hyperparameters}.

\begin{table}[H]
\centering
\caption{Fine-tuning Hyperparameters for ChinaCrimeQACode}
\label{tab:hyperparameters}
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
LoRA rank ($r$) & 64 \\
LoRA alpha & 16 \\
LoRA dropout & 0.0 \\
Batch size (per device) & 32 \\
Gradient accumulation steps & 4 \\
Max sequence length & 3,200 tokens \\
Training epochs & 7 \\
Learning rate & 7e-5 \\
Optimizer & \texttt{adamw\_torch\_fused} \\
Scheduler & \texttt{linear} \\
Warmup ratio & 0.1 \\
Weight decay & 0.01 \\
Quantization & 4-bit (NF4, double quant) \\
\bottomrule
\end{tabular}
\end{table}

\section{Quantitative Results}

\begin{table}[H]
\centering
\caption{Comparison of Evaluation Metrics: Fine-tuned LLaMA3 vs.\ o4-mini (Test Set)}
{%
\begin{tabular}{l|ccc|ccc}
\toprule
\textbf{Metric} & \multicolumn{3}{c|}{\textbf{Fine-tuned LLaMA3}} & \multicolumn{3}{c}{\textbf{o4-mini}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & \textbf{Mean} & \textbf{Std} & \textbf{Median} & \textbf{Mean} & \textbf{Std} & \textbf{Median} \\
\midrule
perc\_error@k\,($\downarrow$)  & 0.233 & 0.299 & 0.125 & 0.003 & 0.014 & 0.000 \\
pass@k\,($\uparrow$)           & 0.742 & 0.437 & 1.000 & 0.303 & 0.459 & 0.000 \\
pass\_over\_k\,($\uparrow$)     & 0.460 & 0.410 & 0.406 & 0.063 & 0.175 & 0.000 \\
code\_bleu@k\,($\uparrow$)     & 0.355 & 0.044 & 0.353 & 0.370 & 0.054 & 0.375 \\
\bottomrule
\end{tabular}
}
\label{tab:combined_metrics}
\end{table}

Based on the observed metrics in Table \ref{tab:combined_metrics}, several conclusions emerge regarding the performance of the fine-tuned LLaMA3‑8B‑Instruct model versus the base o4‑mini. First, LLaMA3‑8B‑Instruct exhibits a higher perc\_error@k than o4‑mini; yet it significantly outperforms o4‑mini in both pass@k and pass\_over\_k. This indicates that, despite a slightly lower rate of successful compilation, the fine‑tuned model produces more precise and semantically relevant solutions. In contrast, o4‑mini's better perc\_error@k can be misleading: it often compiles code that runs but fails to solve the problem correctly.

These findings indicate that broadening and diversifying the crime-domain dataset is essential: a richer corpus will surface edge cases, lower the 0.233 → 0.003 perc\_error@k gap, and lift pass@k beyond its current 0.742 vs 0.303 advantage. We selected o4-mini because its balance of accuracy, cost, and speed makes it a practical deployment benchmark, yet without domain-specific fine-tuning it still trails the task-specific precision achieved by LLaMA3-8B-Instruct, demonstrating the decisive value of targeted data and adaptation.

% Building on the results, we conclude that expanding and diversifying the crime‑domain dataset is imperative: a broader corpus will expose edge cases, reduce perc\_error@k, and allow the model to generalize across varied spatio‑temporal crime patterns, ultimately driving all key metrics upward. We chose o4‑mini as our baseline because its favorable trade‑off among accuracy, computational efficiency, and cost makes it a realistic benchmark for real‑world deployment, yet the current evaluation clearly shows that without domain‑specific fine‑tuning it cannot match the task‑specific precision of LLaMA3‑8B‑Instruct, underscoring the decisive impact of targeted data and adaptation.

% This discrepancy is likely rooted in architectural and training differences. o4‑mini is believed to use a Mixture‑of‑Experts (MoE) setup with approximately 40B total parameters—only about 8B of which are active per inference—whereas LLaMA3‑8B‑Instruct is a dense model with all 8B parameters actively contributing . While MoE architectures provide access to a broader parameter pool, this capacity advantage does not automatically translate to superior domain-specific performance. In contrast, the fully fine‑tuned LLaMA model has internalized domain-specific patterns—such as those needed to manipulate crime data (locations, dates, crime types, etc.)—enabling it to generate more accurate and relevant code.

\section{Qualitative Analysis}

We conducted one case study to illustrate the practical applicationa of our fine-tuned model in real-world scenarios. 

% The first case focuses on counterfactual evaluation of targeted interventions in crime hotspots, while the second explores geospatial queries related to crime data.

\subsection{Case: Counterfactual Evaluation of Targeted Interventions in Crime Hotspots of Guangdong Province}

Zhongshan, a city in Guangdong with around 4.4 million residents (2020), has implemented data-driven policing strategies. Official reports show crime reductions, with a 37\% drop in serious violence and 46\% in theft and robbery by mid-2020 \cite{Zhongshan2020}, highlighting the value of analyzing its crime data for policymaking.

To validate our fine-tuned LLM's capability as an analytical assistant for crime data interpretation, we designed three counterfactual scenarios that test the model's ability to process natural-language queries and generate actionable geospatial insights \cite{William2025}. Each scenario examines different dimensions of crime prevention and resource allocation strategies.



\noindent \textbf{Scenario 1: Night-time Crime Prevention Impact Analysis}

We posed the question: "What percentage reduction in overall crime could be achieved if all night-time thefts in Guangdong Province during 2019 were prevented?" This scenario evaluates the potential impact of enhanced nighttime patrol strategies or improved lighting infrastructure.

The model's analytical approach demonstrates sophisticated understanding of temporal crime patterns. As shown in Listing \ref{lst:q1}, the generated code correctly filters crimes by province and year, implements proper time-based categorization (defining night-time as 6 PM to 6 AM), and calculates the proportional impact of targeted interventions.

\lstinputlisting[style=pythoncode, label={lst:q1}]{codigos/q1.py}

\begin{tcolorbox}[colback=white!95!black, colframe=black, title=Q1 Response, fontupper=\scriptsize, boxsep=2pt, left=3pt, right=3pt]
  The province of Guangdong in China reported 1376 total crimes in 2019. Out of those, 633 crimes occurred at night. If all night-time crimes were prevented, the overall crime rate would decrease by 46.00\%. This is because 46.00\% of the total crimes in 2019 were night-time crimes.
\end{tcolorbox}

The analysis revealed that Guangdong Province recorded 1,376 total crimes in 2019, with 633 occurring during nighttime hours. The model concluded that preventing all night-time crimes would achieve a 46.00\% reduction in overall crime rates, indicating that nearly half of all criminal activity occurs during vulnerable nighttime periods.



\noindent \textbf{Scenario 2: Spatial Crime Diffusion Effects}

The second scenario examined spatial crime relationships: "How would the average crime in neighboring counties (within a 70 km radius of [22.516998, 113.392723]) change if crime at that central point were reduced by 20\%?" This question tests the model's capacity for geospatial reasoning and understanding of crime diffusion effects.

The model's implementation, presented in Listing \ref{lst:q2}, demonstrates interesting geospatial processing capabilities. It correctly creates a buffer zone around the specified coordinates, identifies neighboring counties through geometric intersection analysis, and simulates crime reduction scenarios through statistical sampling.

\lstinputlisting[style=pythoncode, label={lst:q2}]{codigos/q2.py}

\begin{tcolorbox}[colback=white!95!black, colframe=black, title=Q2 Response, fontupper=\scriptsize, boxsep=2pt, left=3pt, right=3pt]
  The average crime rate in neighboring counties would decrease by 13.85\% if crime at the central point were reduced by 20\%. This is because the simulated reduced crime rate in neighboring counties decreased from 4874.79 to 4199.85.
\end{tcolorbox}

The analysis predicted that a 20\% crime reduction at the central hotspot would generate a 13.85\% decrease in average crime rates across neighboring counties. This finding suggests significant spatial spillover effects, where targeted interventions in high-crime areas can produce measurable benefits in surrounding regions.




\noindent \textbf{Scenario 3: Reporting Rate Impact on Safety Rankings}

The final scenario explored data quality implications: "Considering the top 10 safest counties in Guangdong Province, how would this ranking change if crime reporting increased by 15\% in Zhongshan City in 2019?" This question examines how improved reporting mechanisms might alter safety assessments.

The model's approach, illustrated in Listing \ref{lst:q3}, systematically ranks counties by crime frequency, simulates increased reporting rates, and evaluates ranking changes. The analysis revealed that despite a 15\% increase in reported crimes (from 80 to 92 incidents), Zhongshan City maintained its fourth position among the safest counties.

\lstinputlisting[style=pythoncode, label={lst:q3}]{codigos/q3.py}

\begin{tcolorbox}[colback=white!95!black, colframe=black, title=Q3 Response, fontupper=\scriptsize, boxsep=2pt, left=3pt, right=3pt]
  Based on the provided code outputs, the current top 10 safest counties in Guangdong Province for 2019 are:

  1. Bao'an District with 298 crimes

  2. Dongguan City with 139 crimes
  
  3. Baiyun District with 113 crimes
  
  4. Zhongshan City with 80 crimes
  
  5. Longgang District with 78 crimes
  
  6. Panyu District with 62 crimes
  
  7. Shunde District with 61 crimes
  
  8. Chancheng District with 56 crimes
  
  9. Nanshan District with 55 crimes
  
  10. Haizhu District with 49 crimes

  If the crime reporting in Zhongshan City increased by 15\% in 2019, the new top 10 safest counties would be:

  1. Bao'an District with 298 crimes

  2. Dongguan City with 139 crimes

  3. Baiyun District with 113 crimes

  4. Zhongshan City with 92 crimes (after 15\% increase)

  5. Longgang District with 78 crimes

  6. Panyu District with 62 crimes

  7. Shunde District with 61 crimes

  8. Chancheng District with 56 crimes

  9. Nanshan District with 55 crimes

  10. Haizhu District with 49 crimes

  The change in ranking is due to Zhongshan City moving from 4th to 4th place after the increase in crimes.
\end{tcolorbox}

\noindent \textbf{Analytical Implications and Future Potential}

These case studies demonstrate the model's practical utility despite the quantitative metrics presented in Table \ref{tab:combined_metrics}. While our fine-tuned LLaMA3 model shows room for improvement in compilation success rates, the generated code consistently exhibits interesting capabilities in data filtering, temporal analysis, geospatial processing, and statistical simulation—all essential for crime data analysis.

The qualitative analysis reveals that even with current limitations, the model successfully transforms complex criminological questions into executable analytical workflows with contextually appropriate interpretations. This bridges the gap between technical analysis and policy-relevant insights, enabling law enforcement agencies to make informed decisions about resource allocation and intervention strategies.

These promising results suggest that with continued refinement—particularly through expanded domain-specific datasets and targeted improvements to reduce compilation errors—our approach could become a valuable tool for evidence-based policing initiatives. The counterfactual analysis framework validates the potential of fine-tuned LLMs for supporting data-driven decision-making in public safety contexts, especially in regions where technological resources for crime analysis may be limited.


\section{Resume}

