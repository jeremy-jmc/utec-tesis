\chapter{ METHODOLOGY}

\noindent
In this chapter, we present the methodological framework adopted for our study. We detail the processes of data preparation, question-answer generation, model training, and evaluation, as well as the development of the visualization tool. Each section outlines the specific steps and techniques employed to ensure the reliability and reproducibility of our results.

\section{Data preparation}

\noindent
This section describes the procedures followed to curate and preprocess the datasets used in our research. We explain the selection of data sources, the integration of supplementary geographic and road network information, and the steps taken to ensure data quality and consistency prior to analysis.

\subsection{Data sources}

We leveraged the comprehensive China crime dataset published by \cite{Zhang2025CrimeDatasetChina}. The dataset underwent thorough cleaning procedures, including handling of missing values, consolidating category columns through LLM-based classification and standardization of date formats, with all text fields subsequently translated to English. To maintain analytical focus, we concentrated on the three provinces exhibiting the highest crime incidence during the 2017-2019 period: \textit{Jiangsu}, \textit{Guangdong}, and \textit{Zhejiang}. 

The crime dataset was enhanced through integration with publicly available administrative boundary data for China \cite{GeoJSON2025China}, providing polygon-level geographic context. Additionally, we incorporated road network data from OpenStreetMap \cite{Vargas2021OSM} to enable crucial proximity calculations between criminal incidents and nearby roads, a significant variable within our analytical framework. Full dataset columns details are provided in Appendix \ref{appendix:dataset}.

\subsection{Question-Answer generation}

To develop our question-answer dataset, we first created 100 question templates (inspired by \cite{Dai2024QASTKG, Contractor2020QATourism}),  covering different types of inquiries including spatio-temporal, comparative, and causal questions about crime data. These templates were designed to be adaptable for crime datasets from other countries as well.

We expanded this initial set through question augmentation techniques used by \cite{Yin2024MuMathCode, Li2024MuggleMath, Jain2024MetaFineTuning}, specifically rephrasing (with temperature setting of 0.5) and alteration (with temperature setting of 0.75). These question categories are summarized in Table~\ref{tab:question_types}. Then, for each original question type, we selected random examples and developed Python code solutions manually. These hand-crafted solutions provided context for generating additional synthetic code answers using the DeepSeek V3 model with prompting (Appendix~\ref{appendix:prompts}) and nucleus-sampling \cite{Holtzman2020NucleusSampling, Ahmad2025OCRNVidia, Nvidia2024KaggleMath} (top\_p set to 0.95), generating new reasoning paths and coding approaches.

% temperature set to 0.6 and top_p set to 0.95

To ensure quality, we verified that all LLM-generated Python solutions executed correctly. When errors occurred, we resubmitted queries to the LLM until either receiving a working solution or reaching the maximum retry limit. Finally, we conducted manual reviews of all question-answer pairs to guarantee their quality.

The resulting dataset, which we named "ChinaCrimeQACode", contains 5,000 question-code solution pairs, representing an adequate sample volume for effective model training \cite{Unsloth2024Dataset1}. We divided the dataset into training and test subsets using an 80/20 split to facilitate robust model training and comprehensive evaluation.     % , and has been made publicly available

% TODO: poner ejemplo de question-answer pair

\begin{table}[hbtp]
\centering
\caption{Question Types in the ChinaCrimeQACode Dataset}
\label{tab:question_types}
\begin{tabular}{ll}
\toprule
\textbf{Question Type} & \textbf{Description} \\
\midrule
\texttt{spatial\_lookup} & Location-only questions \\
\texttt{temporal\_lookup} & Time-centric questions \\
\texttt{spatio\_temporal\_lookup} & Joint space-time filters \\
\texttt{comparative\_trends} & Rankings and evolution analysis \\
\texttt{causal\_contextual} & Event patterns and conditional queries \\
\texttt{hypotheticals\_counterfactuals} & Projections and counterfactual reasoning \\
\texttt{multi\_step\_aggregation} & Multi-step reasoning and aggregation \\
\bottomrule
\end{tabular}
\end{table}

% This review process involved verifying that the answers were accurate, relevant, and aligned with the corresponding questions. We also ensured that the answers were concise and clear, making them suitable for use in our visualization tool.

\subsection{Model Training}

We fine-tuned the Llama-3.1-8B Instruct model \cite{Grattafiori2024Llama3, Unsloth2024WhatModel} by implementing the techniques described in \cite{Pareja2024RecipesSFT}. For the fine-tuning process, we utilized Hugging Face Transformers alongside the Unsloth library to optimize computational resources and accelerate training, completing the entire procedure in approximately 3 hours using a single NVIDIA H100 GPU with 80GB memory provided by Lightning AI. This model was chosen as it represents a balance between size (8B), data science coding capabilities \cite{Lai2022DS1000}, and compatibility with Unsloth for fast and efficient fine-tuning.

\begin{table}[H]
\centering
\caption{Fine-tuning Hyperparameters for ChinaCrimeQACode}
\label{tab:hyperparameters}
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
LoRA rank ($r$) & 64 \\
LoRA alpha & 16 \\
LoRA dropout & 0.0 \\
Batch size (per device) & 32 \\
Gradient accumulation steps & 4 \\
Max sequence length & 3,200 tokens \\
Training epochs & 7 \\
Learning rate & 7e-5 \\
Optimizer & \texttt{adamw\_torch\_fused} \\
Scheduler & \texttt{linear} \\
Warmup ratio & 0.1 \\
Weight decay & 0.01 \\
Quantization & 4-bit (NF4, double quant) \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Model Evaluation}

% We adopted the approach proposed by \cite{Fleureau2024NuminaMath}, Self-Consistency Tool-Integrated Reasoning (SC-TIR) to evaluate our fine tuned model
For model evaluation, we adopted a comprehensive approach combining both automated and LLM-based assessment methods. Specifically, we used pass@k \cite{Levi2024SimpleModelInferenceScalingLaws} (with $k=16$), where correctness was determined using an LLM-as-a-judge \cite{Li2025LLMJudge} (Appendix~\ref{appendix:prompts}) using GPT-4.1, and CodeBLEU \cite{Ren2020CodeBLEU}, which provides a purely quantitative measure of code generation quality. Additionally, we calculated the error percentage across all generated code samples to assess overall robustness. Finally, we performed Tool Integrated Reasoning (TIR) \cite{Fleureau2024NuminaMath} evaluations, allowing a single retry per question to mitigate frequent inference errors such as ZeroDivisionError and IndexError, while maintaining evaluation efficiency.

% TODO: poner que metodo de inferencia estamos usando, puede que sea greedy decoding

% The model was trained using QLoRA \cite{Dettmers2023QLoRA} with a rank of 64, alpha of 16, and a dropout rate of 0.05. 

% We employed a learning rate of 2e-4 with the AdamW optimizer and cosine learning rate scheduler with 100 warm-up steps. Training was performed for 3 epochs with a batch size of 16, using a maximum sequence length of 2048 tokens. To maintain training stability, we applied gradient clipping with a maximum norm of 1.0.

% For instruction tuning, we formatted our question-code pairs using a standard template that clearly specified the task context and expectations. Each question was prefaced with a system prompt indicating that the model should generate executable Python code to analyze crime data and answer the given question. 


\section{Visualization Tool}

