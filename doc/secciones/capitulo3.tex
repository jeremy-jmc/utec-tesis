\chapter{ METHODOLOGY}

\noindent
This chapter outlines the methodological approach we employed in our research (Figure \ref{fig:methodology_overview}). We describe our procedures for data preparation, the creation of question-code pairs, model training processes, and evaluation strategy. Each section provides detailed explanations of the methods and techniques we used to ensure our findings can be replicated. To maintain transparency, we have included all prompts utilized throughout this study in the Appendix.

% TODO: specify that the dataset is question-code pairs

\begin{figure}[hbtp]
  \centering
  \includegraphics[width=\textwidth]{images/metodologiaExtended.png}
  \captionsetup{justification=raggedright,singlelinecheck=false}
  \caption{Methodology overview: Our approach consists of four main phases. We begin by preprocessing crime data through cleaning and normalization, while integrating geographic boundaries and OpenStreetMap information. We then create a synthetic dataset of question-code pairs using a large language model. This dataset is used to fine-tune a Llama-3.1-8B Instruct model. The final phase involves evaluating model performance and developing an inference pipeline for a practical case study}
  \label{fig:methodology_overview}
\end{figure}


\section{Data preprocessing}

\noindent
This section describes the procedures followed to curate and preprocess the datasets used in our research. We detail the selection of data sources, the integration of supplementary geographic and road network information, and the steps taken to ensure data quality and consistency prior to analysis.


\subsection{Data sources}

Our study utilizes the extensive China crime dataset published by \cite{Zhang2025CrimeDatasetChina}. We selected this dataset due to its substantial volume of data, wide temporal coverage, and broad territorial scope. To provide geographic context, we incorporated administrative boundary data (containing polygon geometries for provinces, cities, and counties) from \cite{GeoJSON2025China} and complemented this with road network graph data from OpenStreetMap (\cite{Vargas2021OSM}). This integration enabled us to calculate spatial relationships between criminal incidents and street polygons. Comprehensive details of all dataset attributes are available in Appendix \ref{appendix:dataset}.

\subsection{Data sources cleaning and normalization}
We cleaned and prepared the dataset through several steps. First, we removed any records from the \texttt{crimes\_df} dataframe that had missing information. We then translated all Chinese text to English using Google Translate to make the data accessible for analysis. Date formats were standardized across all dataframes to ensure proper temporal analysis.

To simplify the analysis, we reduced the number of crime categories by grouping similar types together. We used the DeepSeekV3 language model to classify and merge related crime types into broader, more manageable categories. 

Finally, we focused our study on the three provinces with the most crime incidents between 2017 and 2019: \textit{Jiangsu}, \textit{Guangdong}, and \textit{Zhejiang}. This approach allowed us to work with a more focused dataset while still maintaining sufficient data volume for meaningful analysis.

\section{Question-Answer dataset generation}

% We develop a synthetic dataset named "ChinaCrimesQACode" that contains question-code pairs for crime data analysis. 
This section describes the methodology used to generate the question-code pairs for our synthetic dataset. We detail how we designed the prompts to elicit accurate and functional code solutions from the DeepSeekV3 model, which we used to generate the ground-truth of our dataset.

\subsection{Question generation}

First, we created 100 question templates (inspired by \citep{Dai2024QASTKG, Contractor2020QATourism}), covering different types of inquiries like spatio-temporal, comparative, and causal questions about crime data. These templates were designed to be adaptable for crime datasets from other countries as well.

We expanded this initial set through question augmentation techniques used by \cite{Yin2024MuMathCode, Li2024MuggleMath, Jain2024MetaFineTuning}, specifically rephrasing (with temperature setting of 0.5) and alteration (with temperature setting of 0.75) via few-shot prompting (see prompts in Listings \ref{prompt:question_rephrasing} and \ref{prompt:question_augmentation}). These question categories are summarized in Table~\ref{tab:question_types}. % see Table~\ref{tab:question_type_counts}, 

% \begin{table}[H]
% \centering
% \footnotesize
% \begin{tabular}{lr}
% \hline
% \textbf{Augmentation} & \textbf{\# count} \\
% \hline
% altered     & 3860 \\
% paraphrased & 1732 \\
% \hline
% \end{tabular}
% \caption{Distribution of question types in the dataset.}
% \label{tab:question_type_counts}
% \end{table}

\begin{table}[H]
\centering
\footnotesize
\small
\begin{tabular}{ll}
  \toprule
\textbf{Question Type} & \textbf{Description} \\
\midrule
\texttt{spatial\_lookup} & Location-only questions \\
\texttt{temporal\_lookup} & Time-centric questions \\
\texttt{spatio\_temporal\_lookup} & Joint space-time filters \\
\texttt{comparative\_trends} & Rankings and evolution analysis \\
\texttt{causal\_contextual} & Event patterns and conditional queries \\
\texttt{hypotheticals\_counterfactuals} & Projections and counterfactual reasoning \\
\texttt{multi\_step\_aggregation} & Multi-step reasoning and aggregation \\
\bottomrule
\end{tabular}
\caption{Question types in the ChinaCrimesQACode dataset}
\label{tab:question_types}
\end{table}

\subsection{Human verification}

After generating the augmented questions, we conducted a manual review to ensure their quality and relevance. This process involved checking for grammatical correctness, clarity, alignment with the intended meaning of the original templates, and entities highlighting. We also verified that the questions were appropriate for the crime data context and could be effectively addressed through code solutions. Any questions that did not meet these criteria were either revised or discarded to maintain the integrity of the dataset.


\subsection{Code-answer generation}

For each original question type, we manually wrote one Python code solution to serve as a reference. These hand-crafted examples provided context for generating additional synthetic code answers using the DeepSeekV3 model, guided by a structured prompt and nucleus sampling \citep{Holtzman2020NucleusSampling, Ahmad2025OCRNVidia, Nvidia2024KaggleMath} with a top-p value of 0.95. This allowed the model to explore diverse reasoning paths and alternative implementations.

The answer generation prompt (Figure~\ref{fig:answer_generation_prompt_structure}) follows a layered structure designed to guide the DeepSeekV3 model through a rich, contextualized reasoning process. At the top level, the \textbf{System Instructions} define the model’s role and restrict its scope of reasoning, ensuring consistency and relevance across generations. The \textbf{User Prompt} includes several critical components: the \textit{Dataset Context}, which introduces the \textit{Dataset Specification} of the three core dataframes (\texttt{crimes\_df}, \texttt{streets\_df}, and \texttt{geometries\_df}); one-shot \textit{Reference Material}, offering prior examples (reference question and code) to scaffold the model’s behavior; the target \textit{User Question}; and a \textit{Detailed Task Description} outlining specific rules, especially when the question is a rephrased or altered version of a known query.

Complementing this, the \textbf{Dataset Specification} explicitly lists the column names, sample rows, data types, and semantic descriptions of each field, encouraging schema awareness and reducing ambiguity during code generation. An \textbf{Output Format} section further prescribes the XML response schema to enforce structured and machine-readable outputs.

With this prompt (adaptable to both paraphrased and altered questions), we executed code generation attempts in a loop, verifying each output through assertions, code execution, and consistency checks. If the output failed, we resubmitted the query to the LLM until a valid solution was obtained or the maximum retry limit (set to 3) was reached. Additionally, for paraphrased questions, we used functional equivalence prompts to ensure that newly generated solutions and answers were aligned with the reference implementation.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.65\textwidth]{images/answer_gen.png}
  \caption{Answer generation prompt structure}
  \label{fig:answer_generation_prompt_structure}
\end{figure}


\subsection{Dataset curation and splitting}

To ensure quality, we verified that all LLM-generated Python solutions executed correctly. After that, we dropped the questions with no valid code solutions (limit exceeded, code with errors, or no code generated). Finally, we conducted manual reviews of all question-code pairs to guarantee their quality.

The resulting dataset, which we named ``ChinaCrimesQACodeFunction'', contains around 22,300 question-code solution pairs, representing an adequate sample volume for effective model training \citep{Unsloth2024Dataset1}. We divided the dataset into training and test subsets using an 80/20 split to facilitate robust model training and comprehensive evaluation. The dataset is publicly available on HuggingFace for further research and development.  %  (Table~\ref{tab:dataset_split})
% This review process involved verifying that the answers were accurate, relevant, and aligned with the corresponding questions. We also ensured that the answers were concise and clear, making them suitable for use in our visualization tool.

% \begin{table}[H]
% \centering
% \begin{tabular}{lr}
% \hline
% \textbf{Split} & \textbf{Count} \\
% \hline
% train & 4513 \\
% test  & 1079 \\
% \hline
% \end{tabular}
% \caption{Dataset split distribution.}
% \label{tab:dataset_split}
% \end{table}


% TODO: poner ejemplo de question-code pair

% \section{Dataset generation}

% This section describes the methodology used to generate the question-code pairs for our dataset, focusing on the structured prompts and the code generation process. We detail how we designed the prompts to elicit accurate and functional code solutions from the DeepSeekV3 model, which serves as the backbone for generating answers to user queries.

% \subsection{Prompt Structuration}

% All prompts were carefully designed to ensure the model understood the task requirements and could generate accurate code solutions. We describe below the structure of the prompts used both for answer generation and model training, highlighting how each layer of context helps to elicit precise and functional responses from the model. Full prompt examples are included in Appendix~\ref{appendix:prompts} for transparency and reproducibility.


\section{Training and evaluation}

This section outlines the process of training and evaluating our fine-tuned Llama-3.1-8B Instruct model, covering prompt design, model selection, training setup and assessment metrics used.


\section{Training & evaluation}

This section describes the training and evaluation processes for our fine-tuned Llama-3.1-8B Instruct model, detailing the techniques used for supervised fine-tuning, the evaluation metrics employed, and the overall performance of the model.


\subsection{Training prompt structure}

The training prompt (Figure~\ref{fig:training_prompt_structure}) was designed to fine-tune the model toward robust code generation. It follows a layered structure similar in philosophy to the answer generation prompt (Figure~\ref{fig:answer_generation_prompt_structure}), but introduces stricter constraints and excludes example-based reasoning to encourage generalization. Specifically, the training prompt follows a zero-shot setup, forcing the model to synthesize reasoning paths from schema knowledge and natural language instructions alone, without relying on reference code.

At the top level, the \textbf{System Instructions} define the model’s identity as a geospatial crime analytics expert and restrict the environment to specific, pre-approved libraries. This promotes deterministic and reproducible outputs by reducing spurious library imports and implementation drift.

The \textbf{User Prompt} includes three main elements. First, the \textit{Dataset Context} outlines the structure and semantics of the three core GeoDataFrames, \texttt{crimes\_df}, \texttt{streets\_df}, and \texttt{geometries\_df}, and highlights their interrelations (e.g., spatial joins, containment logic). Second, the \textit{User Question} presents the target query in plain language. Finally, the \textit{Critical Requirements} specify functional and implementation-level constraints, such as required function signatures, alignment of coordinate reference systems (CRS), and the use of robust error handling.

Unlike the answer generation prompt, no reference examples or prior code are provided. This intentional omission reinforces the model's ability to infer behavior solely from structural schema understanding and task requirements.

Lastly, an explicit \textbf{Output Format} section mandates that the model must produce a single Python code block with no additional explanations or comments. This ensures compatibility with downstream evaluators and enables automatic execution. An \textbf{Execution Directive} enforces this behavior from the first token, guaranteeing a clean, code-first output.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.3\textwidth]{images/prompt.png}
  \captionsetup{justification=raggedright,singlelinecheck=false}
  \caption{Training prompt structure: Divided into two main sections. The first section outlines the system instructions, specifying the model's role and permitted libraries. The second section contains the user prompt, which includes the dataset context, user question, and critical requirements.}
  \label{fig:training_prompt_structure}
\end{figure}


\subsection{Model Fine-Tuning}

We fine-tuned the Llama-3.1-8B Instruct model \citep{Grattafiori2024Llama3, Unsloth2024WhatModel} by implementing the techniques described in \cite{Pareja2024RecipesSFT}. For the fine-tuning process, we utilized Hugging Face Transformers alongside the Unsloth library to optimize computational resources and accelerate training, completing the entire procedure in approximately 3 hours using a single NVIDIA H100 GPU with 80GB memory provided by Lightning AI. This model was chosen as it represents a balance between size (8B), data science coding capabilities \citep{Lai2022DS1000}, and compatibility with Unsloth for fast and efficient fine-tuning.



\subsection{Model Evaluation}

% We adopted the approach proposed by \citep{Fleureau2024NuminaMath}, Self-Consistency Tool-Integrated Reasoning (SC-TIR) to evaluate our fine tuned model
For model evaluation, we adopted a comprehensive approach combining both automated and LLM-based assessment methods. Specifically, we used Pass@k and Pass\textasciicircum k \citep{Levi2024SimpleModelInferenceScalingLaws} (with $k=16$ and multinomial sampling), where correctness was determined using an LLM-as-a-judge \citep{Li2025LLMJudge} (Appendix~\ref{appendix:prompts}) using GPT-4.1, and CodeBLEU \citep{Ren2020CodeBLEU}, which provides a purely quantitative measure of code generation quality. Additionally, we calculated the error percentage across all generated code samples to assess overall robustness. Finally, we performed Tool Integrated Reasoning (TIR) \citep{Fleureau2024NuminaMath} evaluations, allowing a single retry per question to mitigate frequent inference errors such as ZeroDivisionError and IndexError, while maintaining evaluation efficiency.


% The model was trained using QLoRA \citep{Dettmers2023QLoRA} with a rank of 64, alpha of 16, and a dropout rate of 0.05. 

% We employed a learning rate of 2e-4 with the AdamW optimizer and cosine learning rate scheduler with 100 warm-up steps. Training was performed for 3 epochs with a batch size of 16, using a maximum sequence length of 2048 tokens. To maintain training stability, we applied gradient clipping with a maximum norm of 1.0.

% For instruction tuning, we formatted our question-code pairs using a standard template that clearly specified the task context and expectations. Each question was prefaced with a system prompt indicating that the model should generate executable Python code to analyze crime data and answer the given question. 


\section{Inference Workflow}

This section describes the inference pipeline implemented for our case studies, detailing the process from user query to final response. As illustrated in Figure~\ref{fig:inference_pipeline}, the workflow consists of several key stages: first, the user query is processed by a fine-tuned Llama3-8B-Instruct model, which generates multiple code solutions. Each code snippet is executed to produce corresponding outputs. Finally, a Llama3-8B-Base model performs summarization of these results to create the final response presented to the user.
%  This architecture enables robust and accurate responses to complex geospatial crime analytics queries.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{images/inference_pipeline.drawio.png}
  \caption{Chat inference pipeline: The user query is first processed by our fine-tuned Llama3-8B-Instruct model that generates multiple code solutions. Each code snippet is executed independently to produce corresponding outputs. Finally, a Llama3-8B-Base model summarizes these results to create a coherent final response presented to the user.}
  \label{fig:inference_pipeline}
\end{figure}

\section{Visualization Tool Integration}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{images/tesis-Visualization tool.png}
  \caption{Visualization tool integration: The user interacts with the map interface to explore crime data insights. While the user makes selections and inputs queries related to the criminality, the backend system processes these requests using our fine-tuned Llama3-8B-Instruct model to generate code solutions. The generated code is executed to retrieve relevant data, which is then visualized on the map interface for user interpretation.}
  \label{fig:visualization_tool_integration}
\end{figure}

To demonstrate the practical application of our fine-tuned model, we integrated it into a web-based visualization tool (Figure~\ref{fig:visualization_tool_integration}). The system architecture employs a conversational interface where users interact with a map to explore crime data insights. The backend processes user queries through a main conversational model that orchestrates the workflow: it interprets natural language inputs, determines when specialized code generation is required, and invokes our fine-tuned model to produce executable solutions. These code outputs are then executed and their results interpreted by the main model to generate comprehensive responses.

This interactive tool enables users to explore crime data through both visual and conversational interfaces. Users can formulate natural language queries about crime analytics, which are processed by our Llama3-8B-Instruct model to generate Python code solutions. The executed code retrieves relevant data that is subsequently visualized on the map, facilitating dynamic exploration of complex geospatial crime datasets.

The visualization tool provides the following map interaction capabilities:
\begin{itemize}
  \item \textbf{Zooming and Panning:} Users can navigate across different geographic scales and regions to examine spatial crime patterns at various levels of detail.
  \item \textbf{Area Selection:} Multiple selection tools (rectangle, polygon, and lasso) enable users to define custom geographic boundaries for filtering and analyzing crime data within specific regions of interest.
  \item \textbf{Chat Interface:} A conversational window processes natural language queries through the fine-tuned Llama3-8B-Instruct model. Based on the query complexity and response requirements, the system may generate Vega-Lite specifications to visualize analytical results as interactive charts and graphs.
\end{itemize}
